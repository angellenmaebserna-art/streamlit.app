# -*- coding: utf-8 -*-
"""microplasticDM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZHPM1aEf49d6HCYk2m4ocl5FzOpuaStC

## Summary:

### Data Analysis Key Findings

*   The average microplastic levels were calculated for each year from 2015 onwards.
*   An ARIMA model with an order of (5, 1, 0) was trained on the yearly microplastic data.
*   The trained ARIMA model was used to forecast microplastic levels for the next three years.
*   The historical and forecasted microplastic levels were visualized in a line plot, showing an increasing trend in forecasted microplastic levels.

### Insights or Next Steps

*   Investigate the `ConvergenceWarning` encountered during ARIMA model fitting to ensure the model's reliability.
*   Explore alternative time series models (e.g., Prophet, Exponential Smoothing) and compare their forecasting performance.
"""

import matplotlib.pyplot as plt
import pandas as pd

# Ensure necessary dataFrames are available
if 'yearly_microplastic' not in locals():
    # Load the dataset
    df = pd.read_csv('merged_microplastic_data.csv')
    # Assuming df is available, aggregate yearly data
    yearly_microplastic = df.groupby('Year')['Microplastic_Level'].mean().reset_index()

# Plot historical data
plt.figure(figsize=(12, 7))
plt.plot(yearly_microplastic['Year'], yearly_microplastic['Microplastic_Level'], marker='o', label='Historical Data')

# Add ARIMA forecasted values to the plot
# Ensure forecast and forecast_steps are available from ARIMA forecasting
if 'forecast' in locals() and 'forecast_steps' in locals():
    last_year = yearly_microplastic['Year'].iloc[-1]
    forecast_years = range(last_year + 1, last_year + forecast_steps + 1)
    # Ensure forecast is a Series with a simple index for plotting
    if isinstance(forecast.index, pd.MultiIndex):
         forecast_to_plot = forecast.reset_index(drop=True)
    else:
         forecast_to_plot = forecast
    plt.plot(forecast_years, forecast_to_plot, marker='x', linestyle='--', color='red', label='ARIMA Forecast')
else:
    print("ARIMA forecast not available. Please run the ARIMA forecasting steps.")


# Add Simple Exponential Smoothing forecasted values to the plot
# Ensure forecast_ses and forecast_steps_ses are available from SES forecasting
if 'forecast_ses' in locals() and 'forecast_steps_ses' in locals():
    last_year = yearly_microplastic['Year'].iloc[-1]
    forecast_years_ses = range(last_year + 1, last_year + forecast_steps_ses + 1)
    # Ensure forecast_ses is a Series with a simple index for plotting
    if isinstance(forecast_ses.index, pd.MultiIndex):
         forecast_ses_to_plot = forecast_ses.reset_index(drop=True)
    else:
         forecast_ses_to_plot = forecast_ses
    plt.plot(forecast_years_ses, forecast_ses_to_plot, marker='o', linestyle='--', color='green', label='SES Forecast')
else:
     print("Simple Exponential Smoothing forecast not available. Please run the SES forecasting steps.")


# Add Prophet forecasted values to the plot
# Ensure forecast_prophet is available from Prophet forecasting
if 'forecast_prophet' in locals():
    # Prophet forecast includes historical and future dates, slice to get only future dates
    last_historical_date = yearly_microplastic['ds'].iloc[-1]
    prophet_future_forecast = forecast_prophet[forecast_prophet['ds'] > last_historical_date]
    plt.plot(prophet_future_forecast['ds'], prophet_future_forecast['yhat'], marker='^', linestyle='-.', color='purple', label='Prophet Forecast')
    # Optionally, add uncertainty intervals
    plt.fill_between(prophet_future_forecast['ds'], prophet_future_forecast['yhat_lower'], prophet_future_forecast['yhat_upper'], color='purple', alpha=0.1)
else:
    print("Prophet forecast not available. Please run the Prophet forecasting steps.")


# Add title and labels
plt.title('Historical and Forecasted Microplastic Levels Over Years (ARIMA, SES, and Prophet)')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')

# Add legend
plt.legend()

# Display the plot
plt.grid(True)
plt.show()

"""## Explore Prophet time series model

### Subtask:
Prepare data for Prophet, train the model, forecast, and visualize the results.
"""

from prophet import Prophet
import pandas as pd

# Prepare data for Prophet: It requires a DataFrame with 'ds' (datetime) and 'y' (value) columns
# Convert the 'Year' column to datetime objects
yearly_microplastic['ds'] = pd.to_datetime(yearly_microplastic['Year'], format='%Y')
yearly_microplastic['y'] = yearly_microplastic['Microplastic_Level']

# Create a new DataFrame with only 'ds' and 'y'
prophet_df = yearly_microplastic[['ds', 'y']]

# Instantiate and fit the Prophet model
model_prophet = Prophet()
model_prophet.fit(prophet_df)

print("Prophet model trained successfully.")

# Create a DataFrame with future dates for forecasting
future = model_prophet.make_future_dataframe(periods=3, freq='Y') # Forecast for the next 3 years, yearly frequency

# Make predictions
forecast_prophet = model_prophet.predict(future)

print("Prophet forecast generated.")
display(forecast_prophet[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())

# Visualize the Prophet forecast
fig = model_prophet.plot(forecast_prophet)
plt.title('Prophet Forecast of Microplastic Levels')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')
plt.show()

fig2 = model_prophet.plot_components(forecast_prophet)

# Forecast microplastic levels for the next 3 years using Simple Exponential Smoothing
forecast_steps_ses = 3
forecast_ses = model_ses_fit.forecast(steps=forecast_steps_ses)

# Display the forecasted values
print(f"Forecasted Microplastic Levels (Simple Exponential Smoothing) for the next {forecast_steps_ses} years:")
display(forecast_ses)

from statsmodels.tsa.holtwinters import SimpleExpSmoothing

# Instantiate and fit the Simple Exponential Smoothing model
# We use the yearly_microplastic DataFrame created earlier
model_ses = SimpleExpSmoothing(yearly_microplastic['Microplastic_Level'])
model_ses_fit = model_ses.fit()

print("Simple Exponential Smoothing model trained successfully.")

"""the three time series models we explored for forecasting microplastic growth.

1. ARIMA Model: We trained an ARIMA model with order (5, 1, 0) on the yearly microplastic data. While the model was fitted and a forecast was generated, we encountered ConvergenceWarnings during fitting and time series cross-validation, suggesting potential issues with model stability given the limited data. The ARIMA forecast showed fluctuating predicted values for the next three years.

2. Simple Exponential Smoothing (SES) Model: As a simpler alternative, we trained a Simple Exponential Smoothing model. This model is suitable for data without a strong trend or seasonality and is often more stable with limited data. The SES forecast predicted a constant microplastic level for the next three years, equal to the last smoothed value.

3. Prophet Model: We also explored the Prophet model, which is designed for time series data with strong seasonality and trend. We prepared the data and fitted the Prophet model, which did not show convergence warnings. The Prophet forecast indicated a decreasing trend in predicted microplastic levels for the next few years.

Comparison: The combined plot shows that the three models provide quite different forecasts for the future. The SES model predicts a stable level, the ARIMA forecast shows fluctuations, and the Prophet model predicts a decreasing trend. The differences in forecasts highlight the uncertainty when predicting with a limited number of historical data points. The convergence warnings with ARIMA suggest that its forecast might be less reliable in this context.

Overall Insight: Due to the limited historical data (yearly data from 2015-2025), it's challenging to definitively determine which model is the most accurate for forecasting future microplastic levels. The forecasts from all models should be interpreted with caution. Exploring more advanced time series techniques or incorporating additional relevant features (if available) could potentially improve forecasting accuracy, but the primary limitation here is the dataset size.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Visualize the correlation matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Numerical Variables')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Distribution of 'Microplastic_Level'
plt.figure(figsize=(10, 6))
sns.histplot(df['Microplastic_Level'], kde=True)
plt.title('Distribution of Microplastic Levels')
plt.xlabel('Microplastic Level')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Microplastic_Level'])
plt.title('Box Plot of Microplastic Levels')
plt.xlabel('Microplastic Level')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Year', y='Microplastic_Level', data=df)
plt.title('Microplastic Level vs. Year')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Longitude', y='Microplastic_Level', data=df)
plt.title('Microplastic Level vs. Longitude')
plt.xlabel('Longitude')
plt.ylabel('Microplastic Level')
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(10, 6))
sns.scatterplot(x='Latitude', y='Microplastic_Level', data=df)
plt.title('Microplastic Level vs. Latitude')
plt.xlabel('Latitude')
plt.ylabel('Microplastic Level')
plt.show()

"""## Load the data

### Subtask:
Load the `merged_microplastic_data.csv` file into a pandas DataFrame.

"""

# Analyze Microplastic_Level for different 'Place' locations
microplastic_by_place = df.groupby('Place')['Microplastic_Level'].agg(['mean', 'median', 'count', 'min', 'max']).reset_index()

# Sort by mean microplastic level for better readability
microplastic_by_place_sorted = microplastic_by_place.sort_values(by='mean', ascending=False)

display(microplastic_by_place_sorted)
print(f"\nNumber of unique place locations: {len(microplastic_by_place_sorted)}")

import matplotlib.pyplot as plt
import seaborn as sns

# Calculate the mean microplastic level per year
mean_microplastic_by_year = df.groupby('Year')['Microplastic_Level'].mean().reset_index()

# Plot the mean microplastic level over the years using a line plot
plt.figure(figsize=(10, 6))
sns.lineplot(x='Year', y='Microplastic_Level', data=mean_microplastic_by_year, marker='o')
plt.title('Mean Microplastic Level Over Years')
plt.xlabel('Year')
plt.ylabel('Mean Microplastic Level')
plt.grid(True)
plt.show()

# Group by Latitude and Longitude and calculate aggregate statistics
microplastic_by_coordinates = df.groupby(['Latitude', 'Longitude'])['Microplastic_Level'].agg(['mean', 'median', 'count']).reset_index()

display(microplastic_by_coordinates.head())
print(f"\nNumber of unique coordinate locations: {len(microplastic_by_coordinates)}")

import pandas as pd

df = pd.read_csv('merged_microplastic_data.csv')

"""**Reasoning**:
Import pandas and load the data into a DataFrame.


"""

import pandas as pd

df = pd.read_csv('merged_microplastic_data.csv')

"""**Reasoning**:
Display the first few rows of the DataFrame to verify the data loading.


"""

display(df.head())

"""## Initial data inspection and cleaning

### Subtask:
Display the first few rows, check for missing values, data types, and basic statistics. Handle missing values and inconsistencies as needed.

**Reasoning**:
Display the first few rows, check data types, non-null values, basic statistics, and missing values as requested in the instructions.
"""

display(df.head())
df.info()
display(df.describe())
display(df.isnull().sum())

"""## Data mining/exploration

### Subtask:
Perform exploratory data analysis (EDA) to understand the data distribution, relationships between variables, and identify potential features for modeling. This might involve visualizations and summary statistics.

**Reasoning**:
Analyze the distribution of 'Microplastic_Level', investigate the relationship between 'Year' and 'Microplastic_Level', explore the relationship between categorical variables ('Site' and 'Place') and 'Microplastic_Level', and examine the geographical distribution of microplastic levels using plots.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Distribution of 'Microplastic_Level'
plt.figure(figsize=(10, 6))
sns.histplot(df['Microplastic_Level'], kde=True)
plt.title('Distribution of Microplastic Levels')
plt.xlabel('Microplastic Level')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x=df['Microplastic_Level'])
plt.title('Box Plot of Microplastic Levels')
plt.xlabel('Microplastic Level')
plt.show()

# 2. Relationship between 'Year' and 'Microplastic_Level'
plt.figure(figsize=(10, 6))
sns.scatterplot(x='Year', y='Microplastic_Level', data=df)
plt.title('Microplastic Level vs. Year')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')
plt.show()

# 3. Relationship between categorical variables and 'Microplastic_Level'
plt.figure(figsize=(12, 6))
sns.boxplot(x='Site', y='Microplastic_Level', data=df)
plt.title('Microplastic Level by Site')
plt.xlabel('Site')
plt.ylabel('Microplastic Level')
plt.show()

plt.figure(figsize=(15, 8))
sns.boxplot(x='Place', y='Microplastic_Level', data=df)
plt.title('Microplastic Level by Place')
plt.xlabel('Place')
plt.ylabel('Microplastic Level')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# 4. Geographical distribution of microplastic levels
plt.figure(figsize=(12, 8))
scatter = plt.scatter(df['Longitude'], df['Latitude'], c=df['Microplastic_Level'], cmap='viridis', s=df['Microplastic_Level']/50)
plt.title('Geographical Distribution of Microplastic Levels')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
cbar = plt.colorbar(scatter)
cbar.set_label('Microplastic Level')
plt.show()

"""**Reasoning**:
Calculate and display correlation coefficients between numerical variables and summarize the key findings from the EDA.


"""

# 5. Correlation coefficients between numerical variables
numerical_cols = ['Latitude', 'Longitude', 'Year', 'Microplastic_Level']
correlation_matrix = df[numerical_cols].corr()
display(correlation_matrix)

# 6. Summarize key findings
print("Summary of EDA Findings:")
print("- The distribution of Microplastic_Level is skewed to the right, indicating a few locations have very high levels.")
print("- There are potential outliers in Microplastic_Level as seen in the box plot.")
print("- The scatter plot of Microplastic_Level vs. Year does not show a clear linear temporal trend.")
print("- Microplastic levels vary significantly across different Sites and Places.")
print("- The geographical scatter plot shows potential spatial patterns in Microplastic_Level, with some clusters of high levels.")
print("- The correlation matrix quantifies the linear relationships between numerical variables.")

"""## Data preprocessing for modeling

### Subtask:
Prepare the data for classification and prediction. This could include feature scaling, encoding categorical variables, and splitting the data into training and testing sets.

**Reasoning**:
Prepare the data for classification by encoding categorical features, defining the target variable by categorizing 'Microplastic_Level', and splitting the data into training and testing sets.
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import pandas as pd # Import pandas here

# 1. Identify features and target variable
# Features: 'Site', 'Place', 'Latitude', 'Longitude', 'Year'
# Target: 'Microplastic_Level' (will be categorized)
features = ['Site', 'Place', 'Latitude', 'Longitude', 'Year']
target = 'Microplastic_Level'

X = df[features]
y = df[target]

# 2. Handle categorical features using one-hot encoding
categorical_features = ['Site', 'Place']
encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
encoded_categorical_features = encoder.fit_transform(X[categorical_features])

# Create a DataFrame with the encoded features
encoded_feature_names = encoder.get_feature_names_out(categorical_features)
encoded_df = pd.DataFrame(encoded_categorical_features, columns=encoded_feature_names, index=X.index)

# Drop original categorical columns and concatenate with encoded features
X = X.drop(categorical_features, axis=1)
X = pd.concat([X, encoded_df], axis=1)

# 3. Define the target variable by categorizing 'Microplastic_Level'
# Determine thresholds based on distribution (e.g., using quantiles or visual inspection)
# Let's use quantiles for demonstration: Low (0-50%), Medium (50-90%), High (90%-100%)
low_threshold = y.quantile(0.5)
medium_threshold = y.quantile(0.9)

def categorize_microplastic_level(level):
    if level <= low_threshold:
        return 'Low'
    elif level <= medium_threshold:
        return 'Medium'
    else:
        return 'High'

y_categorized = y.apply(categorize_microplastic_level)

# Display the distribution of the new target variable
display(y_categorized.value_counts())

# 4. Split the preprocessed data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y_categorized, test_size=0.2, random_state=42, stratify=y_categorized)

print("Data splitting complete.")
print(f"Original data shape: {df.shape}")
print(f"Features shape after encoding: {X.shape}")
print(f"Target shape: {y_categorized.shape}")
print(f"Training features shape: {X_train.shape}")
print(f"Testing features shape: {X_test.shape}")
print(f"Training target shape: {y_train.shape}")
print(f"Testing target shape: {y_test.shape}")

"""## Classification model training

### Subtask:
Choose and train a classification model on the training data.

**Reasoning**:
Import the necessary classification model and train it using the training data.
"""

from sklearn.ensemble import RandomForestClassifier

# Instantiate the model
model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
model.fit(X_train, y_train)

print("Random Forest Classifier model trained successfully.")

# Group by Latitude and Longitude and calculate aggregate statistics
microplastic_by_coordinates = df.groupby(['Latitude', 'Longitude'])['Microplastic_Level'].agg(['mean', 'median', 'count']).reset_index()

display(microplastic_by_coordinates.head())
print(f"\nNumber of unique coordinate locations: {len(microplastic_by_coordinates)}")

"""## Model evaluation

### Subtask:
Evaluate the trained classification model using appropriate metrics on the testing data.

**Reasoning**:
Import necessary evaluation metrics, make predictions on the test set, and evaluate the model using classification report and accuracy score.
"""

from sklearn.metrics import classification_report, accuracy_score

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Generate and display the classification report
classification_rep = classification_report(y_test, y_pred)
print("Classification Report:")
print(classification_rep)

# Calculate and display the accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy Score: {accuracy}")

import folium
from folium.plugins import HeatMap
import pandas as pd

# Ensure microplastic_by_coordinates is available
if 'microplastic_by_coordinates' not in locals():
    # Group by Latitude and Longitude and calculate aggregate statistics
    microplastic_by_coordinates = df.groupby(['Latitude', 'Longitude'])['Microplastic_Level'].agg(['mean', 'median', 'count']).reset_index()

# Create a base map centered around the mean latitude and longitude
mean_latitude = microplastic_by_coordinates['Latitude'].mean()
mean_longitude = microplastic_by_coordinates['Longitude'].mean()
m = folium.Map(location=[mean_latitude, mean_longitude], zoom_start=6)

# Add markers for locations, with size and color indicating microplastic level
for index, row in microplastic_by_coordinates.iterrows():
    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=row['mean'] / 200,  # Scale radius by microplastic level
        color='red' if row['mean'] > microplastic_by_coordinates['mean'].quantile(0.9) else 'blue', # Color high levels red
        fill=True,
        fill_color='red' if row['mean'] > microplastic_by_coordinates['mean'].quantile(0.9) else 'blue',
        fill_opacity=0.6,
        tooltip=f"Level: {row['mean']:.2f}"
    ).add_to(m)

# Display the map
display(m)

"""## Prediction

### Subtask:
Use the trained model to make predictions on new data.

**Reasoning**:
Use the trained classification model to make predictions on the testing features X_test and store the predictions in y_pred.
"""

y_pred = model.predict(X_test)
display(y_pred[:5])

"""## Summary:

### Data Analysis Key Findings

*   The dataset contains 2000 entries and 6 columns, including `Site`, `Place`, `Latitude`, `Longitude`, `Year`, and `Microplastic_Level`.
*   There are no missing values in the dataset.
*   The distribution of `Microplastic_Level` is skewed to the right, indicating a few locations have very high levels and the presence of potential outliers.
*   No clear linear temporal trend was observed between `Year` and `Microplastic_Level`.
*   `Microplastic_Level` varies significantly across different `Site` and `Place` locations, suggesting their importance as features.
*   Potential spatial patterns in `Microplastic_Level` were observed based on `Latitude` and `Longitude`.
*   `Microplastic_Level` was categorized into 'Low' (0-50% quantile), 'Medium' (50-90% quantile), and 'High' (90%-100% quantile) for classification.
*   After one-hot encoding categorical features, the feature set size increased from 6 to 46.
*   A Random Forest Classifier model was trained on the data, achieving an accuracy of 83.25\% on the testing set.
*   The model performed well in predicting the 'Low' and 'Medium' classes (f1-scores of 0.93 and 0.81 respectively) but struggled with the 'High' class (precision and recall of 0.00).

### Insights or Next Steps

*   Investigate alternative approaches for handling the imbalanced 'High' class, such as resampling techniques (oversampling the minority class or undersampling the majority classes) or using evaluation metrics less sensitive to imbalance, like the confusion matrix or Cohen's Kappa.
*   Explore different classification models or hyperparameter tuning for the Random Forest model to potentially improve performance, especially for the 'High' microplastic level prediction.

# Task
Perform prediction analytics, evaluate the model, validate, and perform cross-validation on the `merged_microplastic_data` dataset, providing visualizations of the results.

## Perform cross-validation

### Subtask:
Implement cross-validation to get a more robust estimate of the model's performance.

**Reasoning**:
Implement cross-validation to assess the model's performance more reliably by using `cross_val_score` on the training data and printing the results, including the mean and standard deviation.
"""

from sklearn.model_selection import cross_val_score
import numpy as np

# Perform 5-fold cross-validation on the training data
cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')

# Print the cross-validation scores
print("Cross-validation scores (accuracy) for each fold:")
print(cv_scores)

# Print the mean and standard deviation of the cross-validation scores
print(f"\nMean cross-validation accuracy: {np.mean(cv_scores):.4f}")
print(f"Standard deviation of cross-validation accuracy: {np.std(cv_scores):.4f}")

"""# Task
Predict microplastic growth in the succeeding years using the "merged_microplastic_data.csv" dataset.

## Prepare data for time series analysis

### Subtask:
Aggregate the data to a yearly level and create a time series dataset.

**Reasoning**:
Aggregate the data by year and calculate the mean microplastic level to create a time series dataset, then display the first few rows.
"""

# Group by Year and calculate the mean Microplastic_Level
yearly_microplastic = df.groupby('Year')['Microplastic_Level'].mean().reset_index()

# Display the first few rows of the yearly_microplastic DataFrame
display(yearly_microplastic.head())

"""## Evaluate the time series forecast model

### Subtask:
Evaluate the forecast using appropriate time series metrics.
"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Assuming you have actual values for the forecast period (e.g., in a 'future_years' DataFrame)
# For demonstration, let's assume we have actual values for the 3 forecasted years.
# In a real scenario, you would need actual data for these years to evaluate.

# Since we don't have actual future data, we can evaluate the model's performance on the historical data
# by comparing the model's fitted values to the actual historical values.

# Get the fitted values from the ARIMA model
fitted_values = model_arima_fit.fittedvalues

# Ensure fitted_values and yearly_microplastic['Microplastic_Level'] have the same index and length
# This might require slicing yearly_microplastic to match the fitted_values if the model
# was not fitted on the entire series or if there are missing values handled internally.
# For simplicity here, we assume they align.

# Calculate evaluation metrics on the historical data
mse = mean_squared_error(yearly_microplastic['Microplastic_Level'], fitted_values)
rmse = np.sqrt(mse)
mae = mean_absolute_error(yearly_microplastic['Microplastic_Level'], fitted_values)

print(f"Mean Squared Error (MSE) on historical data: {mse:.4f}")
print(f"Root Mean Squared Error (RMSE) on historical data: {rmse:.4f}")
print(f"Mean Absolute Error (MAE) on historical data: {mae:.4f}")

"""## Time series cross-validation

### Subtask:
Implement time series cross-validation to get a more robust estimate of the model's performance.
"""

from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import make_scorer

# Define the scorer for cross-validation (e.g., negative mean squared error)
# We use negative MSE because cross_val_score maximizes the score.
neg_mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)

# Perform time series cross-validation
# n_splits determines the number of splits. Each split is a test set.
# The training set for each split is all the data before the test set.
tscv = TimeSeriesSplit(n_splits=min(5, len(yearly_microplastic) // 2)) # Use at most 5 splits, and ensure at least 2 data points per split

cv_results = []
for train_index, test_index in tscv.split(yearly_microplastic):
    train_data = yearly_microplastic.iloc[train_index]
    test_data = yearly_microplastic.iloc[test_index]

    # Retrain the model on the training split
    model_arima_cv = ARIMA(train_data['Microplastic_Level'], order=(5, 1, 0))
    model_arima_cv_fit = model_arima_cv.fit()

    # Make predictions on the test split
    forecast_cv = model_arima_cv_fit.forecast(steps=len(test_data))

    # Evaluate the forecast for this split
    mse_cv = mean_squared_error(test_data['Microplastic_Level'], forecast_cv)
    cv_results.append(mse_cv)

print("Time Series Cross-validation results (Mean Squared Error for each split):")
print(cv_results)
print(f"\nMean MSE from time series cross-validation: {np.mean(cv_results):.4f}")
print(f"Standard deviation of MSE from time series cross-validation: {np.std(cv_results):.4f}")

"""## Choose and train a time series model

### Subtask:
Select an appropriate time series model (e.g., ARIMA, Prophet) and train it on the historical data.

**Reasoning**:
Import the ARIMA model and train it on the yearly microplastic data.
"""

from statsmodels.tsa.arima.model import ARIMA

# Instantiate an ARIMA model with order (5, 1, 0)
model_arima = ARIMA(yearly_microplastic['Microplastic_Level'], order=(5, 1, 0))

# Fit the ARIMA model to the data
model_arima_fit = model_arima.fit()

print("ARIMA model trained successfully.")

"""## Forecast future microplastic levels

### Subtask:
Use the trained model to predict microplastic levels for the next 3 or more years.

**Reasoning**:
Use the trained ARIMA model to forecast microplastic levels for the next 3 years and display the forecast.
"""

# Forecast microplastic levels for the next 3 years
forecast_steps = 3
forecast = model_arima_fit.forecast(steps=forecast_steps)

# Display the forecasted values
print(f"Forecasted Microplastic Levels for the next {forecast_steps} years:")
display(forecast)

"""## Visualize the forecast

### Subtask:
Plot the historical data and the forecasted values to visualize the predicted growth.

**Reasoning**:
Create a line plot of the historical data and add the forecasted values with appropriate labels and a legend.
"""

import matplotlib.pyplot as plt

# Plot historical data and ARIMA forecast separately
plt.figure(figsize=(10, 6))
plt.plot(yearly_microplastic['Year'], yearly_microplastic['Microplastic_Level'], marker='o', label='Historical Data')
# Add ARIMA forecasted values to the plot
last_year = yearly_microplastic['Year'].iloc[-1]
forecast_years = range(last_year + 1, last_year + forecast_steps + 1)
plt.plot(forecast_years, forecast, marker='x', linestyle='--', color='red', label='ARIMA Forecast')
plt.title('Historical Data and ARIMA Forecast')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')
plt.legend()
plt.grid(True)
plt.show()

# Plot historical data and Simple Exponential Smoothing forecast separately
plt.figure(figsize=(10, 6))
plt.plot(yearly_microplastic['Year'], yearly_microplastic['Microplastic_Level'], marker='o', label='Historical Data')
# Add Simple Exponential Smoothing forecasted values to the plot
forecast_years_ses = range(last_year + 1, last_year + forecast_steps_ses + 1)
plt.plot(forecast_years_ses, forecast_ses, marker='o', linestyle='--', color='green', label='SES Forecast')
plt.title('Historical Data and Simple Exponential Smoothing Forecast')
plt.xlabel('Year')
plt.ylabel('Microplastic Level')
plt.legend()
plt.grid(True)
plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The average microplastic levels were calculated for each year from 2015 onwards.
*   An ARIMA model with an order of (5, 1, 0) was trained on the yearly microplastic data.
*   The trained ARIMA model was used to forecast microplastic levels for the next three years.
*   The historical and forecasted microplastic levels were visualized in a line plot, showing an increasing trend in forecasted microplastic levels.

### Insights or Next Steps

*   Investigate the `ConvergenceWarning` encountered during ARIMA model fitting to ensure the model's reliability.
*   Explore alternative time series models (e.g., Prophet, Exponential Smoothing) and compare their forecasting performance.

"""